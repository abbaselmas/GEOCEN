{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Melis-GAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5bXQk2DxiEF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "52da576e-78d9-48a4-83ed-062147428fa2"
      },
      "source": [
        "from keras import optimizers\n",
        "from keras.layers import Input, Dense, BatchNormalization, Reshape, LeakyReLU, Flatten\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "\n",
        "\n",
        "class GAN:\n",
        "    def __init__(self):\n",
        "        self.img_rows = 28\n",
        "        self.img_cols = 28\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "\n",
        "        optimizer = optimizers.Adam(0.0002, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "        # Build and compile the generator\n",
        "        self.generator = self.build_generator()\n",
        "        self.generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "        # The generator takes noise as input and generated imgs\n",
        "        z = Input(shape=(100,))\n",
        "        img = self.generator(z)\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # The valid takes generated images as input and determines validity\n",
        "        valid = self.discriminator(img)\n",
        "\n",
        "        # The combined model  (stacked generator and discriminator) takes\n",
        "        # noise as input => generates images => determines validity\n",
        "        self.combined = Model(z, valid)\n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    def build_generator(self):\n",
        "        noise_shape = (100,)\n",
        "        model = Sequential()\n",
        "        model.add(Dense(128, input_shape=noise_shape))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(256, input_shape=noise_shape))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(512, input_shape=noise_shape))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(1024, input_shape=noise_shape))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
        "        model.add(Reshape(self.img_shape))\n",
        "        model.summary()\n",
        "        noise = Input(shape=noise_shape)\n",
        "        img = model(noise)\n",
        "        return Model(noise, img)\n",
        "\n",
        "    def build_discriminator(self):\n",
        "        img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        model = Sequential()\n",
        "        model.add(Flatten(input_shape=img_shape))\n",
        "        model.add(Dense(1024))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(512))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(256))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(128))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        model.summary()\n",
        "        img = Input(shape=img_shape)\n",
        "        validity = model(img)\n",
        "        return Model(img, validity)\n",
        "\n",
        "    def train(self, epochs, batch_size=128):\n",
        "        # Load the dataset\n",
        "        (x_train, _), (_, _) = mnist.load_data()\n",
        "\n",
        "        # Rescale -1 to 1\n",
        "        x_train = (x_train.astype(np.float32) - 127.5) / 127.5\n",
        "        x_train = np.expand_dims(x_train, axis=3)\n",
        "        half_batch = int(batch_size / 2)\n",
        "        for epoch in range(epochs):\n",
        "            idx = np.random.randint(0, x_train.shape[0], half_batch)\n",
        "            imgs = x_train[idx]\n",
        "            noise = np.random.normal(0, 1, (half_batch, 100))\n",
        "            gen_imgs = self.generator.predict(noise)\n",
        "            d_loss_real = self.discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
        "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "            valid_y = np.array([1] * batch_size)\n",
        "            g_loss = self.combined.train_on_batch(noise, valid_y)\n",
        "            if epoch % 100 == 0:\n",
        "              print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100 * d_loss[1], g_loss))\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFf-XO2ixoJn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6341
        },
        "outputId": "d3976975-53f1-4327-9128-30659ef850f5"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    gan = GAN()\n",
        "    gan.train(epochs=30000, batch_size=32)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1024)              803840    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 1,492,993\n",
            "Trainable params: 1,492,993\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 128)               12928     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 784)               803600    \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 1,514,128\n",
            "Trainable params: 1,510,288\n",
            "Non-trainable params: 3,840\n",
            "_________________________________________________________________\n",
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 [D loss: 0.614918, acc.: 46.88%] [G loss: 0.519842]\n",
            "100 [D loss: 1.353316, acc.: 28.12%] [G loss: 6.601250]\n",
            "200 [D loss: 0.566558, acc.: 68.75%] [G loss: 4.982168]\n",
            "300 [D loss: 0.336050, acc.: 96.88%] [G loss: 3.623726]\n",
            "400 [D loss: 0.478535, acc.: 84.38%] [G loss: 2.679579]\n",
            "500 [D loss: 0.518039, acc.: 71.88%] [G loss: 1.425866]\n",
            "600 [D loss: 0.399841, acc.: 87.50%] [G loss: 1.269207]\n",
            "700 [D loss: 0.532370, acc.: 75.00%] [G loss: 1.182909]\n",
            "800 [D loss: 0.467541, acc.: 78.12%] [G loss: 1.704172]\n",
            "900 [D loss: 0.413700, acc.: 84.38%] [G loss: 1.447115]\n",
            "1000 [D loss: 0.484190, acc.: 78.12%] [G loss: 1.584817]\n",
            "1100 [D loss: 0.506371, acc.: 84.38%] [G loss: 1.558793]\n",
            "1200 [D loss: 0.692829, acc.: 62.50%] [G loss: 1.497614]\n",
            "1300 [D loss: 0.424142, acc.: 84.38%] [G loss: 2.286608]\n",
            "1400 [D loss: 0.638762, acc.: 65.62%] [G loss: 1.368561]\n",
            "1500 [D loss: 0.557648, acc.: 84.38%] [G loss: 1.398899]\n",
            "1600 [D loss: 0.323917, acc.: 87.50%] [G loss: 1.617555]\n",
            "1700 [D loss: 0.505550, acc.: 81.25%] [G loss: 1.532552]\n",
            "1800 [D loss: 0.407248, acc.: 78.12%] [G loss: 1.451262]\n",
            "1900 [D loss: 0.488954, acc.: 75.00%] [G loss: 1.579392]\n",
            "2000 [D loss: 0.421580, acc.: 84.38%] [G loss: 1.325603]\n",
            "2100 [D loss: 0.475343, acc.: 75.00%] [G loss: 1.542457]\n",
            "2200 [D loss: 0.592036, acc.: 62.50%] [G loss: 1.341322]\n",
            "2300 [D loss: 0.394888, acc.: 90.62%] [G loss: 1.185610]\n",
            "2400 [D loss: 0.632624, acc.: 68.75%] [G loss: 1.091265]\n",
            "2500 [D loss: 0.460195, acc.: 78.12%] [G loss: 1.376474]\n",
            "2600 [D loss: 0.614173, acc.: 68.75%] [G loss: 1.336629]\n",
            "2700 [D loss: 0.583370, acc.: 65.62%] [G loss: 1.533523]\n",
            "2800 [D loss: 0.538625, acc.: 81.25%] [G loss: 1.262496]\n",
            "2900 [D loss: 0.694164, acc.: 62.50%] [G loss: 1.163327]\n",
            "3000 [D loss: 0.616282, acc.: 65.62%] [G loss: 1.510728]\n",
            "3100 [D loss: 0.540329, acc.: 68.75%] [G loss: 1.203829]\n",
            "3200 [D loss: 0.551524, acc.: 71.88%] [G loss: 1.161563]\n",
            "3300 [D loss: 0.690817, acc.: 62.50%] [G loss: 1.363560]\n",
            "3400 [D loss: 0.512838, acc.: 71.88%] [G loss: 1.361255]\n",
            "3500 [D loss: 0.584110, acc.: 71.88%] [G loss: 1.219665]\n",
            "3600 [D loss: 0.638892, acc.: 68.75%] [G loss: 1.256613]\n",
            "3700 [D loss: 0.583347, acc.: 68.75%] [G loss: 1.461643]\n",
            "3800 [D loss: 0.457988, acc.: 78.12%] [G loss: 1.185195]\n",
            "3900 [D loss: 0.709967, acc.: 65.62%] [G loss: 1.153529]\n",
            "4000 [D loss: 0.668268, acc.: 62.50%] [G loss: 1.116773]\n",
            "4100 [D loss: 0.563912, acc.: 68.75%] [G loss: 1.322229]\n",
            "4200 [D loss: 0.451131, acc.: 81.25%] [G loss: 1.302741]\n",
            "4300 [D loss: 0.460525, acc.: 78.12%] [G loss: 1.335077]\n",
            "4400 [D loss: 0.602575, acc.: 71.88%] [G loss: 1.095273]\n",
            "4500 [D loss: 0.615102, acc.: 68.75%] [G loss: 1.174220]\n",
            "4600 [D loss: 0.439861, acc.: 78.12%] [G loss: 1.032877]\n",
            "4700 [D loss: 0.539682, acc.: 75.00%] [G loss: 1.013958]\n",
            "4800 [D loss: 0.624421, acc.: 75.00%] [G loss: 1.119813]\n",
            "4900 [D loss: 0.539976, acc.: 59.38%] [G loss: 0.974167]\n",
            "5000 [D loss: 0.578224, acc.: 65.62%] [G loss: 1.126922]\n",
            "5100 [D loss: 0.628669, acc.: 59.38%] [G loss: 1.190888]\n",
            "5200 [D loss: 0.522092, acc.: 78.12%] [G loss: 1.019506]\n",
            "5300 [D loss: 0.640587, acc.: 56.25%] [G loss: 1.318451]\n",
            "5400 [D loss: 0.561875, acc.: 81.25%] [G loss: 0.957618]\n",
            "5500 [D loss: 0.490219, acc.: 78.12%] [G loss: 1.059715]\n",
            "5600 [D loss: 0.682581, acc.: 62.50%] [G loss: 0.938167]\n",
            "5700 [D loss: 0.481974, acc.: 84.38%] [G loss: 1.017320]\n",
            "5800 [D loss: 0.567428, acc.: 65.62%] [G loss: 1.150792]\n",
            "5900 [D loss: 0.621345, acc.: 62.50%] [G loss: 0.948654]\n",
            "6000 [D loss: 0.566035, acc.: 78.12%] [G loss: 1.172865]\n",
            "6100 [D loss: 0.625801, acc.: 53.12%] [G loss: 1.072883]\n",
            "6200 [D loss: 0.491311, acc.: 71.88%] [G loss: 1.027708]\n",
            "6300 [D loss: 0.677127, acc.: 59.38%] [G loss: 1.119192]\n",
            "6400 [D loss: 0.441358, acc.: 93.75%] [G loss: 1.118054]\n",
            "6500 [D loss: 0.559412, acc.: 75.00%] [G loss: 0.957512]\n",
            "6600 [D loss: 0.479087, acc.: 71.88%] [G loss: 0.966561]\n",
            "6700 [D loss: 0.526782, acc.: 71.88%] [G loss: 0.934240]\n",
            "6800 [D loss: 0.674842, acc.: 62.50%] [G loss: 1.148910]\n",
            "6900 [D loss: 0.667727, acc.: 65.62%] [G loss: 1.072538]\n",
            "7000 [D loss: 0.597227, acc.: 68.75%] [G loss: 0.963256]\n",
            "7100 [D loss: 0.632330, acc.: 62.50%] [G loss: 1.003675]\n",
            "7200 [D loss: 0.582961, acc.: 62.50%] [G loss: 0.871694]\n",
            "7300 [D loss: 0.511467, acc.: 81.25%] [G loss: 1.145329]\n",
            "7400 [D loss: 0.580197, acc.: 68.75%] [G loss: 1.010875]\n",
            "7500 [D loss: 0.561291, acc.: 78.12%] [G loss: 1.070817]\n",
            "7600 [D loss: 0.766953, acc.: 50.00%] [G loss: 1.125803]\n",
            "7700 [D loss: 0.807814, acc.: 50.00%] [G loss: 1.169421]\n",
            "7800 [D loss: 0.587961, acc.: 68.75%] [G loss: 1.013380]\n",
            "7900 [D loss: 0.690522, acc.: 59.38%] [G loss: 1.029997]\n",
            "8000 [D loss: 0.605544, acc.: 71.88%] [G loss: 1.058102]\n",
            "8100 [D loss: 0.620635, acc.: 68.75%] [G loss: 0.993512]\n",
            "8200 [D loss: 0.606228, acc.: 62.50%] [G loss: 1.112244]\n",
            "8300 [D loss: 0.554699, acc.: 68.75%] [G loss: 1.161739]\n",
            "8400 [D loss: 0.620942, acc.: 59.38%] [G loss: 0.939227]\n",
            "8500 [D loss: 0.530397, acc.: 71.88%] [G loss: 1.029937]\n",
            "8600 [D loss: 0.524064, acc.: 78.12%] [G loss: 1.039648]\n",
            "8700 [D loss: 0.543573, acc.: 78.12%] [G loss: 1.117746]\n",
            "8800 [D loss: 0.752591, acc.: 56.25%] [G loss: 1.012296]\n",
            "8900 [D loss: 0.569615, acc.: 65.62%] [G loss: 0.994866]\n",
            "9000 [D loss: 0.598742, acc.: 68.75%] [G loss: 1.000673]\n",
            "9100 [D loss: 0.616448, acc.: 68.75%] [G loss: 1.051882]\n",
            "9200 [D loss: 0.559241, acc.: 68.75%] [G loss: 0.945600]\n",
            "9300 [D loss: 0.770229, acc.: 56.25%] [G loss: 1.025255]\n",
            "9400 [D loss: 0.672943, acc.: 59.38%] [G loss: 0.968825]\n",
            "9500 [D loss: 0.591211, acc.: 62.50%] [G loss: 1.015249]\n",
            "9600 [D loss: 0.615935, acc.: 68.75%] [G loss: 1.099797]\n",
            "9700 [D loss: 0.751182, acc.: 50.00%] [G loss: 1.078237]\n",
            "9800 [D loss: 0.635501, acc.: 62.50%] [G loss: 1.006984]\n",
            "9900 [D loss: 0.719744, acc.: 53.12%] [G loss: 0.974886]\n",
            "10000 [D loss: 0.498153, acc.: 78.12%] [G loss: 1.139084]\n",
            "10100 [D loss: 0.571631, acc.: 68.75%] [G loss: 0.881906]\n",
            "10200 [D loss: 0.684688, acc.: 65.62%] [G loss: 0.966895]\n",
            "10300 [D loss: 0.623337, acc.: 65.62%] [G loss: 1.001222]\n",
            "10400 [D loss: 0.664298, acc.: 56.25%] [G loss: 1.190508]\n",
            "10500 [D loss: 0.562885, acc.: 68.75%] [G loss: 1.252714]\n",
            "10600 [D loss: 0.648124, acc.: 65.62%] [G loss: 0.950238]\n",
            "10700 [D loss: 0.611160, acc.: 65.62%] [G loss: 1.152985]\n",
            "10800 [D loss: 0.731937, acc.: 53.12%] [G loss: 0.949127]\n",
            "10900 [D loss: 0.650769, acc.: 65.62%] [G loss: 1.129902]\n",
            "11000 [D loss: 0.825119, acc.: 46.88%] [G loss: 1.150959]\n",
            "11100 [D loss: 0.687743, acc.: 56.25%] [G loss: 0.949699]\n",
            "11200 [D loss: 0.545170, acc.: 68.75%] [G loss: 0.950813]\n",
            "11300 [D loss: 0.630183, acc.: 75.00%] [G loss: 1.099036]\n",
            "11400 [D loss: 0.733224, acc.: 68.75%] [G loss: 1.174814]\n",
            "11500 [D loss: 0.648076, acc.: 62.50%] [G loss: 1.050885]\n",
            "11600 [D loss: 0.609590, acc.: 56.25%] [G loss: 1.017603]\n",
            "11700 [D loss: 0.585519, acc.: 71.88%] [G loss: 0.957415]\n",
            "11800 [D loss: 0.729057, acc.: 43.75%] [G loss: 0.957743]\n",
            "11900 [D loss: 0.582493, acc.: 71.88%] [G loss: 1.019175]\n",
            "12000 [D loss: 0.540021, acc.: 71.88%] [G loss: 0.893781]\n",
            "12100 [D loss: 0.633401, acc.: 62.50%] [G loss: 0.879020]\n",
            "12200 [D loss: 0.607089, acc.: 62.50%] [G loss: 0.975895]\n",
            "12300 [D loss: 0.607477, acc.: 65.62%] [G loss: 0.917811]\n",
            "12400 [D loss: 0.598553, acc.: 62.50%] [G loss: 0.977139]\n",
            "12500 [D loss: 0.690068, acc.: 50.00%] [G loss: 1.058894]\n",
            "12600 [D loss: 0.534135, acc.: 68.75%] [G loss: 0.957017]\n",
            "12700 [D loss: 0.617094, acc.: 62.50%] [G loss: 0.945453]\n",
            "12800 [D loss: 0.601173, acc.: 56.25%] [G loss: 1.034697]\n",
            "12900 [D loss: 0.468222, acc.: 81.25%] [G loss: 1.009691]\n",
            "13000 [D loss: 0.722113, acc.: 59.38%] [G loss: 0.967386]\n",
            "13100 [D loss: 0.738038, acc.: 65.62%] [G loss: 1.062326]\n",
            "13200 [D loss: 0.611649, acc.: 59.38%] [G loss: 1.048384]\n",
            "13300 [D loss: 0.682650, acc.: 62.50%] [G loss: 1.201930]\n",
            "13400 [D loss: 0.724917, acc.: 56.25%] [G loss: 1.094561]\n",
            "13500 [D loss: 0.516890, acc.: 75.00%] [G loss: 1.168944]\n",
            "13600 [D loss: 0.615391, acc.: 65.62%] [G loss: 0.949396]\n",
            "13700 [D loss: 0.642288, acc.: 75.00%] [G loss: 1.078397]\n",
            "13800 [D loss: 0.578491, acc.: 65.62%] [G loss: 0.908661]\n",
            "13900 [D loss: 0.586047, acc.: 62.50%] [G loss: 1.015394]\n",
            "14000 [D loss: 0.738433, acc.: 59.38%] [G loss: 1.044952]\n",
            "14100 [D loss: 0.586865, acc.: 71.88%] [G loss: 1.034997]\n",
            "14200 [D loss: 0.564929, acc.: 68.75%] [G loss: 1.009229]\n",
            "14300 [D loss: 0.531430, acc.: 75.00%] [G loss: 0.956012]\n",
            "14400 [D loss: 0.521898, acc.: 78.12%] [G loss: 1.019177]\n",
            "14500 [D loss: 0.691395, acc.: 46.88%] [G loss: 0.951898]\n",
            "14600 [D loss: 0.640862, acc.: 59.38%] [G loss: 1.029805]\n",
            "14700 [D loss: 0.526752, acc.: 78.12%] [G loss: 0.964799]\n",
            "14800 [D loss: 0.551389, acc.: 75.00%] [G loss: 1.024965]\n",
            "14900 [D loss: 0.667045, acc.: 59.38%] [G loss: 1.129777]\n",
            "15000 [D loss: 0.697431, acc.: 46.88%] [G loss: 1.011057]\n",
            "15100 [D loss: 0.583573, acc.: 71.88%] [G loss: 0.935727]\n",
            "15200 [D loss: 0.622735, acc.: 75.00%] [G loss: 0.981047]\n",
            "15300 [D loss: 0.689669, acc.: 56.25%] [G loss: 0.866103]\n",
            "15400 [D loss: 0.521167, acc.: 65.62%] [G loss: 1.091876]\n",
            "15500 [D loss: 0.581964, acc.: 71.88%] [G loss: 0.992664]\n",
            "15600 [D loss: 0.742963, acc.: 46.88%] [G loss: 1.005833]\n",
            "15700 [D loss: 0.596068, acc.: 65.62%] [G loss: 1.021618]\n",
            "15800 [D loss: 0.610358, acc.: 71.88%] [G loss: 1.020005]\n",
            "15900 [D loss: 0.648044, acc.: 59.38%] [G loss: 1.223116]\n",
            "16000 [D loss: 0.637992, acc.: 59.38%] [G loss: 1.025676]\n",
            "16100 [D loss: 0.657624, acc.: 59.38%] [G loss: 1.052594]\n",
            "16200 [D loss: 0.688312, acc.: 59.38%] [G loss: 1.155852]\n",
            "16300 [D loss: 0.611190, acc.: 59.38%] [G loss: 1.059122]\n",
            "16400 [D loss: 0.476746, acc.: 81.25%] [G loss: 1.066272]\n",
            "16500 [D loss: 0.558299, acc.: 65.62%] [G loss: 0.960850]\n",
            "16600 [D loss: 0.643273, acc.: 56.25%] [G loss: 1.055206]\n",
            "16700 [D loss: 0.631810, acc.: 62.50%] [G loss: 1.051294]\n",
            "16800 [D loss: 0.633509, acc.: 56.25%] [G loss: 1.030242]\n",
            "16900 [D loss: 0.514945, acc.: 78.12%] [G loss: 0.935485]\n",
            "17000 [D loss: 0.656378, acc.: 46.88%] [G loss: 1.087065]\n",
            "17100 [D loss: 0.744055, acc.: 59.38%] [G loss: 1.082203]\n",
            "17200 [D loss: 0.663468, acc.: 65.62%] [G loss: 0.976076]\n",
            "17300 [D loss: 0.536977, acc.: 75.00%] [G loss: 0.903455]\n",
            "17400 [D loss: 0.633453, acc.: 68.75%] [G loss: 0.893799]\n",
            "17500 [D loss: 0.640066, acc.: 53.12%] [G loss: 1.084199]\n",
            "17600 [D loss: 0.600526, acc.: 62.50%] [G loss: 1.003608]\n",
            "17700 [D loss: 0.637911, acc.: 53.12%] [G loss: 0.968978]\n",
            "17800 [D loss: 0.716541, acc.: 53.12%] [G loss: 0.986883]\n",
            "17900 [D loss: 0.626996, acc.: 59.38%] [G loss: 0.912310]\n",
            "18000 [D loss: 0.647745, acc.: 65.62%] [G loss: 0.898198]\n",
            "18100 [D loss: 0.658367, acc.: 68.75%] [G loss: 0.999855]\n",
            "18200 [D loss: 0.617533, acc.: 59.38%] [G loss: 1.051160]\n",
            "18300 [D loss: 0.623021, acc.: 59.38%] [G loss: 1.033584]\n",
            "18400 [D loss: 0.649962, acc.: 68.75%] [G loss: 0.980313]\n",
            "18500 [D loss: 0.583545, acc.: 68.75%] [G loss: 1.048660]\n",
            "18600 [D loss: 0.576222, acc.: 68.75%] [G loss: 1.014714]\n",
            "18700 [D loss: 0.602190, acc.: 59.38%] [G loss: 0.884249]\n",
            "18800 [D loss: 0.558517, acc.: 75.00%] [G loss: 1.050540]\n",
            "18900 [D loss: 0.605559, acc.: 65.62%] [G loss: 0.800542]\n",
            "19000 [D loss: 0.653216, acc.: 65.62%] [G loss: 0.872657]\n",
            "19100 [D loss: 0.755595, acc.: 56.25%] [G loss: 0.912514]\n",
            "19200 [D loss: 0.563360, acc.: 65.62%] [G loss: 0.895965]\n",
            "19300 [D loss: 0.718172, acc.: 56.25%] [G loss: 0.970182]\n",
            "19400 [D loss: 0.645445, acc.: 62.50%] [G loss: 1.013363]\n",
            "19500 [D loss: 0.673765, acc.: 62.50%] [G loss: 0.995410]\n",
            "19600 [D loss: 0.654616, acc.: 68.75%] [G loss: 1.077771]\n",
            "19700 [D loss: 0.626693, acc.: 75.00%] [G loss: 1.111835]\n",
            "19800 [D loss: 0.598237, acc.: 59.38%] [G loss: 1.191321]\n",
            "19900 [D loss: 0.655105, acc.: 59.38%] [G loss: 1.109673]\n",
            "20000 [D loss: 0.634660, acc.: 75.00%] [G loss: 1.022309]\n",
            "20100 [D loss: 0.594767, acc.: 62.50%] [G loss: 1.019576]\n",
            "20200 [D loss: 0.620565, acc.: 59.38%] [G loss: 1.051233]\n",
            "20300 [D loss: 0.603312, acc.: 65.62%] [G loss: 0.941081]\n",
            "20400 [D loss: 0.604100, acc.: 78.12%] [G loss: 1.027107]\n",
            "20500 [D loss: 0.607097, acc.: 68.75%] [G loss: 1.016046]\n",
            "20600 [D loss: 0.534839, acc.: 65.62%] [G loss: 1.096365]\n",
            "20700 [D loss: 0.451930, acc.: 84.38%] [G loss: 1.211223]\n",
            "20800 [D loss: 0.736616, acc.: 46.88%] [G loss: 0.970905]\n",
            "20900 [D loss: 0.700198, acc.: 43.75%] [G loss: 0.906037]\n",
            "21000 [D loss: 0.684515, acc.: 56.25%] [G loss: 1.022366]\n",
            "21100 [D loss: 0.552935, acc.: 71.88%] [G loss: 1.133724]\n",
            "21200 [D loss: 0.507204, acc.: 71.88%] [G loss: 1.090551]\n",
            "21300 [D loss: 0.672627, acc.: 46.88%] [G loss: 1.030335]\n",
            "21400 [D loss: 0.700610, acc.: 62.50%] [G loss: 1.101246]\n",
            "21500 [D loss: 0.646611, acc.: 62.50%] [G loss: 0.978546]\n",
            "21600 [D loss: 0.708384, acc.: 56.25%] [G loss: 1.007908]\n",
            "21700 [D loss: 0.672821, acc.: 56.25%] [G loss: 0.958273]\n",
            "21800 [D loss: 0.600552, acc.: 59.38%] [G loss: 1.056849]\n",
            "21900 [D loss: 0.673111, acc.: 50.00%] [G loss: 0.949475]\n",
            "22000 [D loss: 0.587101, acc.: 68.75%] [G loss: 0.997248]\n",
            "22100 [D loss: 0.639667, acc.: 65.62%] [G loss: 1.094426]\n",
            "22200 [D loss: 0.579419, acc.: 71.88%] [G loss: 0.975049]\n",
            "22300 [D loss: 0.649657, acc.: 62.50%] [G loss: 0.957880]\n",
            "22400 [D loss: 0.675354, acc.: 56.25%] [G loss: 0.986891]\n",
            "22500 [D loss: 0.756959, acc.: 56.25%] [G loss: 1.005141]\n",
            "22600 [D loss: 0.546410, acc.: 71.88%] [G loss: 1.236400]\n",
            "22700 [D loss: 0.627708, acc.: 59.38%] [G loss: 1.152612]\n",
            "22800 [D loss: 0.677515, acc.: 53.12%] [G loss: 1.137704]\n",
            "22900 [D loss: 0.639875, acc.: 59.38%] [G loss: 1.009737]\n",
            "23000 [D loss: 0.550126, acc.: 71.88%] [G loss: 1.056792]\n",
            "23100 [D loss: 0.609183, acc.: 62.50%] [G loss: 1.000102]\n",
            "23200 [D loss: 0.689065, acc.: 53.12%] [G loss: 1.084339]\n",
            "23300 [D loss: 0.652064, acc.: 53.12%] [G loss: 1.016279]\n",
            "23400 [D loss: 0.532550, acc.: 65.62%] [G loss: 1.197538]\n",
            "23500 [D loss: 0.662893, acc.: 62.50%] [G loss: 1.036906]\n",
            "23600 [D loss: 0.560932, acc.: 68.75%] [G loss: 0.971142]\n",
            "23700 [D loss: 0.623572, acc.: 71.88%] [G loss: 1.035586]\n",
            "23800 [D loss: 0.734605, acc.: 53.12%] [G loss: 0.985815]\n",
            "23900 [D loss: 0.618356, acc.: 65.62%] [G loss: 1.017380]\n",
            "24000 [D loss: 0.678853, acc.: 50.00%] [G loss: 0.986549]\n",
            "24100 [D loss: 0.500460, acc.: 75.00%] [G loss: 1.099532]\n",
            "24200 [D loss: 0.644636, acc.: 59.38%] [G loss: 0.933262]\n",
            "24300 [D loss: 0.610278, acc.: 62.50%] [G loss: 1.006651]\n",
            "24400 [D loss: 0.449291, acc.: 84.38%] [G loss: 1.071553]\n",
            "24500 [D loss: 0.547574, acc.: 75.00%] [G loss: 0.972860]\n",
            "24600 [D loss: 0.533704, acc.: 75.00%] [G loss: 1.177913]\n",
            "24700 [D loss: 0.689591, acc.: 59.38%] [G loss: 1.088978]\n",
            "24800 [D loss: 0.731237, acc.: 46.88%] [G loss: 1.090444]\n",
            "24900 [D loss: 0.643840, acc.: 62.50%] [G loss: 1.056833]\n",
            "25000 [D loss: 0.644511, acc.: 65.62%] [G loss: 0.954634]\n",
            "25100 [D loss: 0.562802, acc.: 71.88%] [G loss: 0.920081]\n",
            "25200 [D loss: 0.651629, acc.: 71.88%] [G loss: 1.068052]\n",
            "25300 [D loss: 0.772654, acc.: 59.38%] [G loss: 1.175817]\n",
            "25400 [D loss: 0.579934, acc.: 65.62%] [G loss: 0.978784]\n",
            "25500 [D loss: 0.599404, acc.: 62.50%] [G loss: 1.028280]\n",
            "25600 [D loss: 0.597081, acc.: 71.88%] [G loss: 1.191104]\n",
            "25700 [D loss: 0.621897, acc.: 68.75%] [G loss: 1.023808]\n",
            "25800 [D loss: 0.562227, acc.: 71.88%] [G loss: 1.099118]\n",
            "25900 [D loss: 0.549038, acc.: 71.88%] [G loss: 1.127121]\n",
            "26000 [D loss: 0.640843, acc.: 62.50%] [G loss: 0.932354]\n",
            "26100 [D loss: 0.580936, acc.: 68.75%] [G loss: 1.055911]\n",
            "26200 [D loss: 0.638277, acc.: 65.62%] [G loss: 1.021523]\n",
            "26300 [D loss: 0.526897, acc.: 75.00%] [G loss: 1.046755]\n",
            "26400 [D loss: 0.511168, acc.: 75.00%] [G loss: 1.008095]\n",
            "26500 [D loss: 0.787166, acc.: 40.62%] [G loss: 1.048117]\n",
            "26600 [D loss: 0.492710, acc.: 78.12%] [G loss: 1.067164]\n",
            "26700 [D loss: 0.525331, acc.: 62.50%] [G loss: 0.978641]\n",
            "26800 [D loss: 0.601902, acc.: 75.00%] [G loss: 0.906188]\n",
            "26900 [D loss: 0.520943, acc.: 68.75%] [G loss: 1.080398]\n",
            "27000 [D loss: 0.590788, acc.: 68.75%] [G loss: 1.052269]\n",
            "27100 [D loss: 0.543753, acc.: 71.88%] [G loss: 1.072439]\n",
            "27200 [D loss: 0.583712, acc.: 56.25%] [G loss: 0.957398]\n",
            "27300 [D loss: 0.636917, acc.: 59.38%] [G loss: 1.050129]\n",
            "27400 [D loss: 0.594942, acc.: 65.62%] [G loss: 1.004892]\n",
            "27500 [D loss: 0.514802, acc.: 81.25%] [G loss: 1.047790]\n",
            "27600 [D loss: 0.522396, acc.: 71.88%] [G loss: 1.180754]\n",
            "27700 [D loss: 0.570585, acc.: 71.88%] [G loss: 0.921827]\n",
            "27800 [D loss: 0.691963, acc.: 65.62%] [G loss: 1.073555]\n",
            "27900 [D loss: 0.607584, acc.: 75.00%] [G loss: 0.925019]\n",
            "28000 [D loss: 0.584700, acc.: 68.75%] [G loss: 1.017822]\n",
            "28100 [D loss: 0.678354, acc.: 59.38%] [G loss: 0.964087]\n",
            "28200 [D loss: 0.636440, acc.: 62.50%] [G loss: 1.015813]\n",
            "28300 [D loss: 0.608310, acc.: 59.38%] [G loss: 1.054671]\n",
            "28400 [D loss: 0.606901, acc.: 62.50%] [G loss: 1.113881]\n",
            "28500 [D loss: 0.648870, acc.: 56.25%] [G loss: 1.023321]\n",
            "28600 [D loss: 0.609287, acc.: 59.38%] [G loss: 1.173948]\n",
            "28700 [D loss: 0.605785, acc.: 71.88%] [G loss: 0.959049]\n",
            "28800 [D loss: 0.668846, acc.: 71.88%] [G loss: 0.900314]\n",
            "28900 [D loss: 0.589137, acc.: 71.88%] [G loss: 1.042373]\n",
            "29000 [D loss: 0.568126, acc.: 75.00%] [G loss: 1.003242]\n",
            "29100 [D loss: 0.669259, acc.: 62.50%] [G loss: 0.942944]\n",
            "29200 [D loss: 0.565099, acc.: 75.00%] [G loss: 1.040709]\n",
            "29300 [D loss: 0.610796, acc.: 65.62%] [G loss: 0.979539]\n",
            "29400 [D loss: 0.566434, acc.: 71.88%] [G loss: 1.092043]\n",
            "29500 [D loss: 0.617765, acc.: 56.25%] [G loss: 1.010595]\n",
            "29600 [D loss: 0.626515, acc.: 56.25%] [G loss: 1.079557]\n",
            "29700 [D loss: 0.671659, acc.: 59.38%] [G loss: 0.900358]\n",
            "29800 [D loss: 0.558506, acc.: 71.88%] [G loss: 0.958440]\n",
            "29900 [D loss: 0.655593, acc.: 50.00%] [G loss: 1.216994]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}